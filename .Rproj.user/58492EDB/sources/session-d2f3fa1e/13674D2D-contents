library(ggplot2)
library(numDeriv)

###############################
# Compound Poisson Normal MLE #
###############################
# find_Kmax <- function(lambda, epsilon = 1e-6) {
#   Kmax <- 0
#   tail_prob <- 1 - ppois(Kmax, lambda)
#   
#   while (tail_prob > epsilon) {
#     Kmax <- Kmax + 1
#     tail_prob <- 1 - ppois(Kmax, lambda)
#   }
#   
#   return(Kmax)
# }
# 
# # Function to compute Hessian using central differences
# hessian_fd <- function(func, x, ..., eps = 1e-5) {
#   n <- length(x)
#   H <- matrix(0, n, n)
#   fx <- func(x, ...)
#   
#   for (i in 1:n) {
#     for (j in i:n) {
#       x_ij_pp <- x_ij_pm <- x_ij_mp <- x_ij_mm <- x
#       
#       x_ij_pp[i] <- x_ij_pp[i] + eps
#       x_ij_pp[j] <- x_ij_pp[j] + eps
#       
#       x_ij_pm[i] <- x_ij_pm[i] + eps
#       x_ij_pm[j] <- x_ij_pm[j] - eps
#       
#       x_ij_mp[i] <- x_ij_mp[i] - eps
#       x_ij_mp[j] <- x_ij_mp[j] + eps
#       
#       x_ij_mm[i] <- x_ij_mm[i] - eps
#       x_ij_mm[j] <- x_ij_mm[j] - eps
#       
#       H[i, j] <- (func(x_ij_pp, ...) - func(x_ij_pm, ...) - 
#                     func(x_ij_mp, ...) + func(x_ij_mm, ...)) / (4 * eps^2)
#       
#       if (i != j) {
#         H[j, i] <- H[i, j]  # exploit symmetry
#       }
#     }
#   }
#   
#   return(H)
# }

# hessian_fd_nosym <- function(func, x, ..., eps = 1e-5) {
#   n <- length(x)
#   H <- matrix(0, n, n)
#   
#   for (i in 1:n) {
#     for (j in 1:n) {
#       x_ij_pp <- x_ij_pm <- x_ij_mp <- x_ij_mm <- x
#       
#       x_ij_pp[i] <- x_ij_pp[i] + eps
#       x_ij_pp[j] <- x_ij_pp[j] + eps
#       
#       x_ij_pm[i] <- x_ij_pm[i] + eps
#       x_ij_pm[j] <- x_ij_pm[j] - eps
#       
#       x_ij_mp[i] <- x_ij_mp[i] - eps
#       x_ij_mp[j] <- x_ij_mp[j] + eps
#       
#       x_ij_mm[i] <- x_ij_mm[i] - eps
#       x_ij_mm[j] <- x_ij_mm[j] - eps
#       
#       H[i, j] <- (func(x_ij_pp, ...) - func(x_ij_pm, ...) -
#                     func(x_ij_mp, ...) + func(x_ij_mm, ...)) / (4 * eps^2)
#     }
#   }
#   
#   return(H)
# }

cpn_regression_neg_log_likelihood <- function(beta_mu_sigma, X, y, Kmax=50) {
  

  # Extract parameters
  p <- ncol(X)
  beta <- beta_mu_sigma[1:p]        # Regression coefficients for lambda
  mu <- beta_mu_sigma[p + 1]        # Mean of normal components
  sigma <- beta_mu_sigma[p + 2]     # Std dev of normal components
  
  if (sigma <= 0) return(Inf)       # Penalize invalid sigma
  
  # Linear predictor for lambda (log-link)
  eta <- X %*% beta
  lambda_vec <- as.vector(exp(eta))  # Observation-specific lambda
  
  # Determine a maximum K across all lambda (for truncation)
  #Kmax <- find_Kmax(max(lambda_vec))
  
  # Compute likelihood for each observation
  likelihoods <- mapply(function(x_i, lambda_i) {
    
    if (x_i == 0) {
      prob <- dpois(0, lambda_i)
    } else {
      k_vals <- 1:Kmax
      poisson_probs <- dpois(k_vals, lambda_i)
      normal_probs <- dnorm(x_i, mean = k_vals * mu, sd = sqrt(k_vals * sigma^2))
      prob <- sum(poisson_probs * normal_probs)
    }
    
    return(prob)
    
  }, y, lambda_vec)
  
  # If any likelihood is 0 (log(0) = -Inf), return Inf for the negative log-likelihood
  if (any(likelihoods <= 0)) return(Inf)
  
  return(-sum(log(likelihoods)))
}


# cpn_reg_mle <- function(X, y) {
#   # X: Design matrix (n x p)
#   # y: Observed response vector (length n)
#   
#   n <- length(y)
#   p <- ncol(X)
#   
#   # Initial values:
#   init_beta <- rep(0, p)              # Start with 0 for all regression coefficients
#   init_mu <- mean(y)           # Mean of observations as proxy for mu
#   init_sigma <- sd(y)  # Std dev of observations as proxy for sigma
#   
#   init_vals <- c(init_beta, init_mu, init_sigma)  # Concatenate all initial values
#   
#   # Optimization
#   mle_result <- optim(
#     par = init_vals,
#     fn = cpn_regression_neg_log_likelihood,
#     X = X,
#     y = y,
#     method = "Nelder-Mead",
#     control = list(maxit = 1000)  # You can tweak this for better convergence
#   )
#   
#   return(mle_result$par)  # Returns c(beta_hat, mu_hat, sigma_hat)
# }


cpn <- function(formula, data = NULL, mu_init = NULL, sigma_init = NULL, epsilon = 1e-6, Kmax=10) {
   
  if (Kmax<10 & Kmax >100) stop("Kmax should be between 10 and 100")
  

  # Ensure required package
   if (!requireNamespace("numDeriv", quietly = TRUE)) {
     stop("Package 'numDeriv' is required but not installed.")
   }
  

  # Build model frame and design matrix (allow formula to work with or without data argument)
  formula <- stats::as.formula(formula, env = parent.frame())
  mf <- stats::model.frame(formula = formula, data = data)
  y <- model.response(mf)
  X <- model.matrix(attr(mf, "terms"), data = mf)
  
  # Initial values
  y_nonzero <- y
  if (is.null(mu_init)) mu_init <- mean(y_nonzero)
  if (is.null(sigma_init)) sigma_init <- sd(y_nonzero)
  init_beta <- rep(0, ncol(X))
  init_vals <- c(init_beta, mu_init, sigma_init)
  
  # Optimize negative log-likelihood
  fit <- optim(
    par = init_vals,
    fn = cpn_regression_neg_log_likelihood,
    X = X,
    y = y,
    Kmax = Kmax,
    method = "Nelder-Mead",
    control = list(maxit = 1000)
  )
  
  beta_hat <- fit$par
  loglik <- -fit$value
  
  # Compute Hessian and standard errors
  # H <- numDeriv::hessian(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  #H <- hessian_fd(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  
  H <- tryCatch({
    numDeriv::hessian(func = function(par) cpn_regression_neg_log_likelihood(par, X, y, Kmax = Kmax),
            x = beta_hat)
  }, error = function(e) {
    warning("Failed to compute Hessian: ", e$message)
    return(matrix(NA, length(beta_hat), length(beta_hat)))
  })
  
 # H <- hessian_fd_nosym(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  if (any(is.na(H)) || det(H) == 0 || any(!is.finite(H))) {
    warning("Hessian is singular or contains non-finite values; SEs are not available.")
    se_hat <- rep(NA, length(beta_hat))
  } else {
    eigs <- eigen(H, symmetric = TRUE)$values
    if (any(eigs <= 0)) {
      warning("Hessian is not positive definite; SEs may be unreliable.")
      se_hat <- sqrt(diag(solve(H)))
    } else {
      se_hat <- sqrt(diag(solve(H)))
    }
  }
  

  param_names <- c(colnames(X), "mu", "sigma")
  names(beta_hat) <- param_names
  names(se_hat) <- param_names
  
  # # Print parameter estimates with SEs
  # cat("Parameter estimates with standard errors:\n")
  # print(data.frame(Estimate = beta_hat, SE = se_hat, row.names = param_names))
  
  # Compute fitted values (linear predictor)
  eta <- as.vector(X %*% beta_hat[1:ncol(X)])
  lambda_hat <- exp(eta)  # Poisson mean
  mu_hat <- beta_hat["mu"]
  sigma_hat <- beta_hat["sigma"]
  
  # Compute individual log-likelihood contributions
  loglik_obs <- function(y_i, lambda_i) {
    if (y_i == 0) {
      log_p0 <- dpois(0, lambda_i)
      return(log(log_p0))
    } else {
      # Convolution of Poisson and Normal (approximate): use numerical integration or approximation
      # We'll use simplified approximation for residuals:
      expected <- lambda_i * mu_hat
      var <- lambda_i * sigma_hat^2
      return(dnorm(y_i, mean = expected, sd = sqrt(var), log = TRUE))
    }
  }
  
  loglik_saturated <- function(y_i) {
    # In saturated model, y_i is predicted perfectly → log-lik is 0 if Normal
    # For residuals, we assume perfect fit implies maximum likelihood → return 0
    return(0)
  }
  
  dev_res <- numeric(length(y))
  for (i in seq_along(y)) {
    ll_hat <- loglik_obs(y[i], lambda_hat[i])
    ll_sat <- loglik_saturated(y[i])
    dev_res[i] <- sign(y[i] - lambda_hat[i] * mu_hat) * sqrt(2 * (ll_sat - ll_hat))
  }
  
  # Null model (intercept only)
  X_null <- matrix(1, nrow = nrow(X), ncol = 1)
  colnames(X_null) <- "(Intercept)"
  null_fit <- optim(
    par = c(0, mu_init, sigma_init),
    fn = cpn_regression_neg_log_likelihood,
    X = X_null,
    y = y,
    method = "Nelder-Mead",
    Kmax = Kmax,
    control = list(maxit = 1000)
  )
  null_loglik <- -null_fit$value
  
  null_deviance <- -2 * null_loglik
  residual_deviance <- -2 * loglik
  df_null <- length(y) - 1
  df_residual <- length(y) - length(beta_hat)
  
  # AIC: -2 * log-likelihood + 2 * number of parameters
  k <- length(beta_hat)
  aic_val <- 2 * k + 2 * fit$value  # equivalent to -2 * loglik + 2k
  
  model_frame <- model.frame(formula, data)
  terms_obj <- terms(model_frame)
  
  # Add to returned list
  structure(list(
    coefficients = setNames(beta_hat[1:ncol(X)], colnames(X)),
    mu = beta_hat["mu"],
    sigma = beta_hat["sigma"],
    se = se_hat,
    model = model_frame,
    terms = terms_obj,
    formula = formula,
    data = data,
    deviance_residuals = dev_res,
    fitted_values = lambda_hat * mu_hat,
    neg_log_likelihood = fit$value,
    null_deviance = null_deviance,
    residual_deviance = residual_deviance,
    df_null = df_null,
    df_residual = df_residual,
    aic = aic_val,
    call = match.call()
  ), class = "cpn")
}

predict.cpn <- function(object, newdata = NULL, type = c("response", "link"), ...) {
  type <- match.arg(type)
  
  # Remove response from terms to avoid 'y not found' error
  terms_noy <- delete.response(object$terms)
  
  if (is.null(newdata)) {
    X <- stats::model.matrix(terms_noy, data = object$model)
  } else {
    mf <- stats::model.frame(terms_noy, data = newdata)
    X <- stats::model.matrix(terms_noy, data = mf)
  }
  
  beta_hat <- object$coefficients
  eta <- as.vector(X %*% beta_hat)
  
  if (type == "link") {
    return(eta)
  } else if (type == "response") {
    lambda_hat <- exp(eta)
    mu_hat <- object$mu
    return(lambda_hat * mu_hat)
  }
}

summary.cpn <- function(object, ...) {
  beta_hat <- object$coefficients
  se_hat <- object$se[1:length(beta_hat)]
  
  z_vals <- beta_hat / se_hat
  p_vals <- 2 * pnorm(-abs(z_vals))
  param_names <- names(beta_hat)
  
  summary_table <- data.frame(
    Estimate = beta_hat,
    Std.Error = se_hat,
    z.value = z_vals,
    Pr.z = p_vals,
    row.names = param_names
  )
  
  dev_res <- object$deviance_residuals
  dev_summary <- quantile(dev_res, probs = c(0, 0.25, 0.5, 0.75, 1))
  names(dev_summary) <- c("Min", "1Q", "Median", "3Q", "Max")
  
  out <- list(
    call = object$call,
    summary_table = summary_table,
    deviance_summary = dev_summary,
    mu = object$mu,
    sigma = object$sigma,
    null_deviance = object$null_deviance,
    residual_deviance = object$residual_deviance,
    df_null = object$df_null,
    df_residual = object$df_residual,
    aic = object$aic
  )
  class(out) <- "summary.cpn"
  return(out)
}

print.summary.cpn <- function(x, ...) {
  cat("Call:\n")
  print(x$call)
  
  cat("\nDeviance Residuals:\n")
  print(x$deviance_summary, digits = 4, quote = FALSE)
  
  cat("\nCoefficients:\n")
  printCoefmat(x$summary_table, P.values = TRUE, has.Pvalue = TRUE)
  
  cat(sprintf("\nEstimated mu parameter: %.4f\n", x$mu))
  cat(sprintf("Estimated sigma parameter: %.4f\n", x$sigma))
  
  cat(sprintf("\nNull deviance: %.2f on %d degrees of freedom\n",
              x$null_deviance, x$df_null))
  cat(sprintf("Residual deviance: %.2f on %d degrees of freedom\n",
              x$residual_deviance, x$df_residual))
  cat(sprintf("AIC: %.2f\n", x$aic))
  
  invisible(x)
}
# summary.cpn <- function(object, ...) {
#   # Separate mu and sigma from coefficients
#   beta_hat <- object$coefficients
#   se_hat <- object$se[1:length(beta_hat)]
#   
#   # z-values and p-values for coefficients only
#   z_vals <- beta_hat / se_hat
#   p_vals <- 2 * pnorm(-abs(z_vals))
#   
#   param_names <- names(beta_hat)
#   
#   summary_table <- data.frame(
#     Estimate = beta_hat,
#     Std.Error = se_hat,
#     z.value = z_vals,
#     Pr.z = p_vals,
#     row.names = param_names
#   )
#   
#   # Deviance residuals summary
#   dev_res <- object$deviance_residuals
#   dev_summary <- quantile(dev_res, probs = c(0, 0.25, 0.5, 0.75, 1))
#   names(dev_summary) <- c("Min", "1Q", "Median", "3Q", "Max")
#   
#   cat("Call:\n")
#   print(object$call)
#   
#   cat("\nDeviance Residuals:\n")
#   print(dev_summary, digits = 4, quote = FALSE)
#   
#   cat("\nCoefficients:\n")
#   printCoefmat(summary_table, P.values = TRUE, has.Pvalue = TRUE)
#   
#   # Print mu and sigma separately
#   cat(sprintf("\nEstimated mu parameter: %.4f\n", object$mu))
#   cat(sprintf("Estimated sigma parameter: %.4f\n", object$sigma))
#   
#   # Deviance summary
#   cat(sprintf("\nNull deviance: %.2f on %d degrees of freedom\n",
#               object$null_deviance, object$df_null))
#   cat(sprintf("Residual deviance: %.2f on %d degrees of freedom\n",
#               object$residual_deviance, object$df_residual))
#   cat(sprintf("AIC: %.2f\n", object$aic))
#   
#   invisible(summary_table)
# }

# plot.cpn <- function(x, ...) {
#   if (is.null(x$deviance_residuals) || is.null(x$fitted_values)) {
#     stop("Model must include deviance_residuals and fitted_values for plotting.")
#   }
#   
#   par(mfrow = c(2, 2))
#   
#   # 1. Residuals vs Fitted
#   plot(x$fitted_values, x$deviance_residuals,
#        xlab = "Fitted values", ylab = "Deviance residuals",
#        main = "Residuals vs Fitted")
#   abline(h = 0, col = "red", lty = 2)
#   
#   # 2. Q-Q Plot
#   qqnorm(x$deviance_residuals, main = "Normal Q-Q")
#   qqline(x$deviance_residuals, col = "red")
#   
#   # 3. Histogram
#   hist(x$deviance_residuals, breaks = 20, main = "Histogram of Residuals",
#        xlab = "Deviance residuals", col = "gray")
#   
#   # 4. Scale-Location (sqrt(|resid|) vs fitted)
#   sqrt_abs_resid <- sqrt(abs(x$deviance_residuals))
#   plot(x$fitted_values, sqrt_abs_resid,
#        xlab = "Fitted values", ylab = expression(sqrt("|Deviance residuals|")),
#        main = "Scale-Location")
#   abline(h = median(sqrt_abs_resid), col = "red", lty = 2)
#   
#   par(mfrow = c(1, 1))
# }

plot_observed_vs_fitted.cpn <- function(model, ...) {
  if (is.null(model$fitted_values)) {
    stop("Model does not contain fitted values.")
  }
  
  y <- model.response(model.frame(model$formula, data = model$data))
  
  # Save only modifiable graphical parameters
  old_mar <- par("mar")
  old_mfrow <- par("mfrow")
  on.exit({
    par(mar = old_mar)
    par(mfrow = old_mfrow)
  })
  
  # Set default margins and layout
  par(mar = c(5, 5, 4, 2) + 0.1)
  par(mfrow = c(1, 1))
  
  plot(model$fitted_values, y,
       xlab = "Fitted values",
       ylab = "Observed values",
       main = "Observed vs Fitted Values",
       pch = 19, col = "darkgreen")
  abline(a = 0, b = 1, col = "blue", lty = 2)
}

anova.cpn <- function(object, ..., test = "Chisq") {
  # Collect all models
  models <- list(object, ...)
  
  models <- list(fit, fit2)
  n_models <- length(models)
  
  # Check all are of class 'cpn'
  if (!all(sapply(models, inherits, "cpn"))) {
    stop("All objects must be of class 'cpn'")
  }
  
  # Extract model stats
  df_res <- sapply(models, function(m) m$df_residual)
  dev <- sapply(models, function(m) m$residual_deviance)
  n <- length(models[[1]]$data[[1]])
  
  # Sort by model complexity (smaller df_res => more complex)
  ord <- order(df_res)
  models <- models[ord]
  df_res <- df_res[ord]
  dev <- dev[ord]
  
  # Compute differences
  df_diff <- c(NA, diff(df_res))
  dev_diff <- c(NA, diff(dev))
  
  # p-values for Chi-squared test
  p_vals <- c(NA, pchisq(dev_diff[-1], df_diff[-1], lower.tail = FALSE))
  
  # Build result table
  result <- data.frame(
    Residual_Df = df_res,
    Residual_Deviance = dev,
    Df = df_diff,
    Deviance = dev_diff,
    Pr_Chisq = p_vals
  )
  rownames(result) <- paste0("Model ", seq_len(n_models))
  
  
  class(result) <- c("anova", "data.frame")
  return(result)
}


#Print out
print.cpn <- function(x, ...) {
  cat("Call:\n")
  print(x$call)
  
  cat("\nCoefficients:\n")
  print(round(x$coefficients, 4))
  
  cat(sprintf("\nmu: %.4f\n", x$mu))
  cat(sprintf("sigma: %.4f\n", x$sigma))
  
  cat(sprintf("\nResidual deviance: %.2f on %d degrees of freedom\n",
              x$residual_deviance, x$df_residual))
  cat(sprintf("AIC: %.2f\n", x$aic))
  
  invisible(x)
}

coef.cpn <- function(object, full = TRUE, ...) {
  if (full) {
    return(c(object$coefficients, mu = object$mu, sigma = object$sigma))
  }
  return(object$coefficients)
}

fitted.cpn <- function(object, ...) {
  return(object$fitted_values)
}

residuals.cpn <- function(object, type = c("deviance", "raw"), ...) {
  type <- match.arg(type)
  
  if (type == "deviance") {
    return(object$deviance_residuals)
  } else if (type == "raw") {
    return(object$data[[as.character(object$formula[[2]])]] - object$fitted_values)
  }
}

# Step 5: plot.cpn
#' @method plot cpn
#' @export
plot.cpn <- function(x, which = c("residuals", "qq"), ...) {
  which <- match.arg(which)
  par(mfrow = c(1, 1))
  if (which == "residuals") {
    plot(x$fitted_values, x$deviance_residuals,
         xlab = "Fitted values",
         ylab = "Deviance residuals",
         main = "Residuals vs Fitted")
    abline(h = 0, col = "red", lty = 2)
  } else if (which == "qq") {
    qqnorm(x$deviance_residuals)
    qqline(x$deviance_residuals, col = "red")
  }
}

# Step 6: logLik.cpn
#' @method logLik cpn
#' @export
logLik.cpn <- function(object, ...) {
  val <- -object$neg_log_likelihood
  attr(val, "df") <- length(object$se)
  class(val) <- "logLik"
  return(val)
}

# Step 7: AIC.cpn / BIC.cpn
#' @method AIC cpn
#' @export
AIC.cpn <- function(object, ..., k = 2) {
  k * length(object$se) + 2 * object$neg_log_likelihood
}

#' @method BIC cpn
#' @export
BIC.cpn <- function(object, ...) {
  log(nrow(object$data)) * length(object$se) + 2 * object$neg_log_likelihood
}

# Step 8: vcov.cpn
#' @method vcov cpn
#' @export
vcov.cpn <- function(object, ...) {
  # Full parameter vector (coefficients + mu + sigma)
  full_params <- c(object$coefficients, mu = object$mu, sigma = object$sigma)
  
  # Compute Hessian
  H <- numDeriv::hessian(
    cpn_regression_neg_log_likelihood,
    x = full_params,
    X = model.matrix(object$formula, object$data),
    y = model.response(model.frame(object$formula, object$data))
  )
  
  # Check Hessian validity
  if (any(is.na(H)) || det(H) == 0 || any(!is.finite(H))) {
    warning("Hessian is singular or contains non-finite values; vcov not available.")
    return(matrix(NA, nrow = length(full_params), ncol = length(full_params),
                  dimnames = list(names(full_params), names(full_params))))
  }
  
  # Return named variance-covariance matrix
  V <- solve(H)
  dimnames(V) <- list(names(full_params), names(full_params))
  return(V)
}


# Step 9: update.cpn
#' @method update cpn
#' @export
update.cpn <- function(object, formula., data = NULL, ...) {
  new_formula <- update.formula(object$formula, formula.)
  new_data <- if (!is.null(data)) data else object$data
  cpn(new_formula, data = new_data, ...)
}




recover_data.cpn <- function(object, ...) {
  emmeans:::recover_data.call(
    object$call,
    trms = delete.response(object$terms),
    na.action = na.omit,
    data = object$data
  )
}


emm_basis.cpn <- function(object, trms, xlev, grid, ...) {
  # Construct model matrix from reference grid
  X <- model.matrix(trms, grid)
  
  # Extract regression coefficients only (exclude mu and sigma)
  bhat <- object$coefficients  # Named vector
  
  # Extract full vcov and reduce to regression part
  V_full <- vcov(object)
  
  # Sanity check: only keep rows/cols matching bhat
  V <- V_full[names(bhat), names(bhat), drop = FALSE]
  
  # Match X columns with coefficient names
  if (!all(names(bhat) %in% colnames(X))) {
    stop("Mismatch between model matrix columns and coefficient names")
  }
  
  # Ensure X has same columns and order as bhat
  X <- X[, names(bhat), drop = FALSE]
  
  # No penalized/nuisance terms
  nbasis <- matrix(0, 0, length(bhat))
  colnames(nbasis) <- names(bhat)
  
  # Degrees of freedom handling
  dfargs <- list(df = if (!is.null(object$df_residual)) object$df_residual else Inf)
  dffun <- function(k, dfargs) dfargs$df
  
  list(X = X, bhat = bhat, V = V, nbasis = nbasis, dffun = dffun, dfargs = dfargs)
}

###############
# Test model  #
###############

simulate_cpn_data <- function(n = 200, beta = c(0.5, -0.3, 0.7), mu = 2, sigma = 1) {
  set.seed(123)  # For reproducibility
  
  # Simulate predictors
  x1 <- factor(sample(c("A", "B"), size = n, replace = TRUE))  # Categorical
  x2 <- rnorm(n)  # Continuous
  
  # Create model matrix (includes intercept and dummy variable for x1)
  X <- model.matrix(~ x1 + x2)  # Will generate intercept, x1B, x2
  
  # Compute linear predictor and Poisson rates
  eta <- X %*% beta
  lambda <- exp(eta)
  
  # Simulate response variable
  y <- numeric(n)
  for (i in 1:n) {
    k <- rpois(1, lambda[i])
    if (k > 0) {
      y[i] <- sum(rnorm(k, mean = mu, sd = sigma))
    } else {
      y[i] <- 0
    }
  }
  
  # Return as a data.frame
  data.frame(y = y, x1 = x1, x2 = x2)
}

df2 <- simulate_cpn_data()
head(df2)


fit <- cpn(y ~ x1 + x2, data = df2)

coef(fit)

summary(fit)

print(fit)

coef(fit)

fitted(fit)

coef(fit, full = TRUE) # Includes mu and sigma

plot(fit)

logLik(fit)

AIC(fit)

BIC(fit)

vcov(fit)

plot_observed_vs_fitted.cpn(fit)

residuals(fit)

# Remove a variable (e.g., 'hp')
fit2 <- update(fit, . ~ . - x2)

anova(fit, fit2)

fit$data

fit3 <- cpn(y ~ x1 + x2, data = df2)

fit3$vcov
new_df=data.frame(x1= c("A", "A", "B", "B"), x2=c(-0.5, -0.2,-0.3, -0.3))
predict(fit3, type = "response")         # Fitted values on original data
predict(fit3, newdata = new_df)          # Predictions on new data (default: response)
predict(fit3, newdata = new_df, type = "link")  # Linear predictor η on new data

predict.cpn <- function(object, newdata = NULL, type = c("response", "link"),
                        interval = c("none", "confidence"), level = 0.95, ...) {

  
  type <- match.arg(type)
  interval <- match.arg(interval)
  
  terms_noy <- delete.response(object$terms)
  
  if (is.null(newdata)) {
    X <- model.matrix(terms_noy, data = object$model,
                      contrasts.arg = attr(object$model, "contrasts"))
  } else {
    for (v in names(object$model)) {
      if (is.factor(object$model[[v]]) && v %in% names(newdata)) {
        newdata[[v]] <- factor(newdata[[v]], levels = levels(object$model[[v]]))
      }
    }
    mf <- model.frame(terms_noy, data = newdata,
                      xlev = .getXlevels(object$terms, object$model))
    X <- model.matrix(terms_noy, data = mf,
                      contrasts.arg = attr(object$model, "contrasts"))
  }
  
  beta_hat <- object$coefficients
  eta <- as.vector(X %*% beta_hat)
  mu_hat <- object$mu
  se_mu <- object$se["mu"]
  z <- qnorm(1 - (1 - level) / 2)
  
  if (interval == "none") {
    if (type == "link") return(eta)
    if (type == "response") return(exp(eta) * mu_hat)
  }
  
  # Confidence intervals
  V_full <- vcov(object)
  beta_names <- names(object$coefficients)  # only beta, not mu/sigma
  V_beta <- V_full[beta_names, beta_names, drop = FALSE]
  se_eta <- sqrt(rowSums((X %*% V_beta) * X))
  
  if (type == "link") {
    lwr <- eta - z * se_eta
    upr <- eta + z * se_eta
    return(data.frame(fit = eta, lwr = lwr, upr = upr))
  }
  
  # For response: include SE(mu) in delta method
  exp_eta <- exp(eta)
  fit <- exp_eta * mu_hat
  se_fit <- sqrt(
    (mu_hat * exp_eta)^2 * se_eta^2 +
      (exp_eta)^2 * se_mu^2
  )
  
  lwr <- fit - z * se_fit
  upr <- fit + z * se_fit
  return(data.frame(fit = fit, lwr = lwr, upr = upr))
}

predict(fit3, type = "response", interval="confidence")         # Fitted values on original data
predict(fit3, newdata = new_df)          # Predictions on new data (default: response)
predict(fit3, newdata = new_df, type = "link")  # Linear predictor η on new data



# True parameters
true_beta <- c(0, -0.3)
true_mu <- 2
true_sigma <- 1.2
n_sim <- 1000
coverage <- numeric(n_sim)
graphics.off()

for (i in 1:n_sim) {
  
  set.seed(123 +i)
  # Simulate covariates
  x1 <- rbinom(100, 1, 0.5)
  X <- model.matrix(~ x1 )
  eta <- X %*% true_beta
  lambda <- exp(eta)
  
  # Simulate data from CPN model
  N <- rpois(100, lambda)
  y <- sapply(N, function(n) if (n == 0) 0 else sum(rnorm(n, mean = true_mu, sd = true_sigma)))
  data_i <- data.frame(y = y, x1 = as.factor(x1))
  
  # Fit model
  fit <- cpn(y ~ x1, data = data_i)
  
  # Predict at one test point (say, mean covariates)
  newdata <- data.frame(x1 = factor(0, levels = c(0,1)))
  pred <- predict(fit, newdata, interval = "confidence")
  
  # True expected value at this covariate
  eta_true <- true_beta[1] + true_beta[2] * 0
  lambda_true <- exp(eta_true)
  y_expected <- lambda_true * true_mu
  
  # Check if true expected value is in CI
  coverage[i] <- (y_expected >= pred$lwr) & (y_expected <= pred$upr)
  
  if (i %% 10 == 0) cat(i, "\n")  # only print every 10 iterations
}

mean(coverage)  # Should be close to 0.95 for a 95% CI

# 
# If it's too low (e.g., 0.88), your intervals are too narrow (undercover).
# 
# If it's too high (e.g., 0.99), they're too wide (overconservative).

# # True parameters
# true_beta <- c(0.5, -0.3, 0.3)
# true_mu <- 2
# true_sigma <- 1.2
# n_sim <- 1000
# coverage <- numeric(n_sim)
# 
# for (i in 1:n_sim) {
#   # Simulate covariates
#   set.seed(123+i)
#   x1 <- rbinom(100, 1, 0.5)
#   x2 <- rnorm(100)
#   X <- model.matrix(~ x1 + x2)
#   eta <- X %*% true_beta
#   lambda <- exp(eta)
# 
#   # Simulate data from CPN model
#   N <- rpois(100, lambda)
#   y <- sapply(N, function(n) if (n == 0) 0 else sum(rnorm(n, mean = true_mu, sd = true_sigma)))
#   data_i <- data.frame(y = y, x1 = as.factor(x1), x2 = x2)
#   
#   # Fit model
#   fit <- cpn(y ~ x1 + x2, data = data_i)
#   
#   # Predict at one test point (say, mean covariates)
#   newdata <- data.frame(x1 = factor(0, levels = c(0,1)), x2 = 0)
#   pred <- predict(fit, newdata, interval = "confidence")
#   
#   # True expected value at this covariate
#   eta_true <- true_beta[1] + true_beta[2] * 0
#   lambda_true <- exp(eta_true)
#   y_expected <- lambda_true * true_mu
#   
#   # Check if true expected value is in CI
#   coverage[i] <- (y_expected >= pred$lwr) & (y_expected <= pred$upr)
# }
# 

# 0.975
# mean(coverage)  # Should be close to 0.95 for a 95% CI
# 
# If it's too low (e.g., 0.88), your intervals are too narrow (undercover).
# 
# If it's too high (e.g., 0.99), they're too wide (overconservative).



vcov(fit)


summary(fit)


emmeans(fit, pairwise ~x1)
###########################
# Model validation
###########################
coef_list <- NULL 
for (i in c(1:10)){
  simulate_cpn_data <- function(n = 200, beta = c(0.5, -0.3, 0.7), mu = 2, sigma = 1) {
    set.seed(123 +i)  # For reproducibility
    
    # Simulate predictors
    x1 <- factor(sample(c("A", "B"), size = n, replace = TRUE))  # Categorical
    x2 <- rnorm(n)  # Continuous
    
    # Create model matrix (includes intercept and dummy variable for x1)
    X <- model.matrix(~ x1 + x2)  # Will generate intercept, x1B, x2
    
    # Compute linear predictor and Poisson rates
    eta <- X %*% beta
    lambda <- exp(eta)
    
    # Simulate response variable
    y <- numeric(n)
    for (i in 1:n) {
      k <- rpois(1, lambda[i])
      if (k > 0) {
        y[i] <- sum(rnorm(k, mean = mu, sd = sigma))
      } else {
        y[i] <- 0
      }
    }
    
    # Return as a data.frame
    data.frame(y = y, x1 = x1, x2 = x2)
  }
  
  df2 <- simulate_cpn_data()
  head(df2)
  
  
  fit <- cpn(y ~ x1 + x2, data = df2)
  
  coef_list[[i]]<- coef(fit)
  print(i)
}


coef_list

combined_df <- do.call(rbind, coef_list)

summary(combined_df)

m1 <- cpn(y ~ 1, data = df2)
summary(m1)
m2 <- cpn(y ~ x1 + x2, data = df2)
summary(m2)

anova(m1, m2)

####################
# Check regression #
####################
# Function to simulate a Compound Poisson-Normal dataset
simulate_cpn <- function(n, lambda, mu, sigma) {
  counts <- rpois(n, lambda)  # Draw the number of events (k) for each of the n observations from a Poisson distribution
  data <- sapply(counts, function(k) sum(rnorm(k, mean = mu, sd = sigma)))  # For each k, simulate k independent normal variables and sum them
  return(data)  # Return the vector of compound outcomes
}
set.seed(123)  # Set seed for reproducibility
sim_data1 <- simulate_cpn(n = 200, lambda = 1.3, mu = 2, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution
sim_data2 <- simulate_cpn(n = 100, lambda = 1, mu = 2, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution

# Combine data and group labels
sim_data_pooled <- c(sim_data1, sim_data2)
group <- factor(c(rep(1, length(sim_data1)), rep(2, length(sim_data2))))

df_sim <- data.frame( y=sim_data_pooled,
                      group=group)

cpn_fit <- cpn(y~group, data=df_sim)
summary(cpn_fit)

cpn_fit <-cpn(sim_data_pooled~group)
summary(cpn_fit)

##### Negative mu
set.seed(123)  # Set seed for reproducibility
sim_data1 <- simulate_cpn(n = 200, lambda = 1.3, mu = -4, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution
sim_data2 <- simulate_cpn(n = 100, lambda = 1, mu = -4, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution

# Combine data and group labels
sim_data_pooled <- c(sim_data1, sim_data2)
group <- factor(c(rep(1, length(sim_data1)), rep(2, length(sim_data2))))

df_sim <- data.frame( y=sim_data_pooled,
                      group=group)

cpn_fit <- cpn(y~group, data=df_sim)
summary(cpn_fit)

##### Negative mu
set.seed(123)  # Set seed for reproducibility
sim_data1 <- simulate_cpn(n = 1000, lambda = 1.3, mu = -400, sigma = 400)  # Generate 1000 observations from a Compound Poisson-Normal distribution
sim_data2 <- simulate_cpn(n = 1000, lambda = 1, mu = -400, sigma = 400)  # Generate 1000 observations from a Compound Poisson-Normal distribution

# Combine data and group labels
sim_data_pooled <- c(sim_data1, sim_data2)
group <- factor(c(rep(1, length(sim_data1)), rep(2, length(sim_data2))))

df_sim <- data.frame( y=sim_data_pooled,
                      group=group)

cpn_fit <- cpn(y~group, data=df_sim)
names(summary(cpn_fit))

summary(cpn_fit)$call
summary(cpn_fit)$summary_table
summary(cpn_fit)$deviance_summary
summary(cpn_fit)$mu
summary(cpn_fit)$sigma
summary(cpn_fit)$null_deviance
summary(cpn_fit)$residual_deviance
summary(cpn_fit)$df_null
summary(cpn_fit)$df_residual
summary(cpn_fit)$aic    

library(emmeans)

resemm <- emmeans(cpn_fit, ~group)
exp(as.data.frame(resemm)$emmean)

summary(cpn_fit)
cpn_fit$coefficients
emm_basis.cpn <- function(object, trms, xlev, grid, ...) {
  # Construct model matrix from reference grid
  X <- model.matrix(trms, grid)
  
  # Extract regression coefficients only (exclude mu and sigma)
  bhat <- object$coefficients  # Named vector
  
  # Extract full vcov and reduce to regression part
  V_full <- vcov(object)
  V <- V_full[names(bhat), names(bhat), drop = FALSE]
  
  # Match X columns with coefficient names
  if (!all(names(bhat) %in% colnames(X))) {
    stop("Mismatch between model matrix columns and coefficient names")
  }
  X <- X[, names(bhat), drop = FALSE]
  
  # No penalized/nuisance terms
  nbasis <- matrix(0, 0, length(bhat))
  colnames(nbasis) <- names(bhat)
  
  # Extract scalar μ from object (adjust this line as needed)
  mu <- object$mu 
  
  
  # Degrees of freedom
  dfargs <- list(df = if (!is.null(object$df_residual)) object$df_residual else Inf)
  dffun <- function(k, dfargs) dfargs$df
  
  # Specify transformation for emmeans on response scale
  linkinv <- exp  # Inverse of log link
  varfun <- function(var) var^2  # Variance function on log scale (used by emmeans)
  
  list(
    X = X,
    bhat = bhat,
    V = V,
    nbasis = nbasis,
    dffun = dffun,
    dfargs = dfargs,
    misc = list(
      tran = "log",        # Tell emmeans that this is a log-link model
      inv.link = linkinv  ,  # How to go back to response scale
      var.fun = varfun     # Variance function
    )
  )
}


coef(cpn_fit)
emmeans(cpn_fit, ~group, type="response")
emmeans(fit, pairwise~x1, df=Inf)


###
emm_basis.cpn <- function(object, trms, xlev, grid, ...) {
  # Model matrix for linear predictor η = Xβ (log(λ))
  X <- model.matrix(trms, grid)
  
  # Regression coefficients for log(λ)
  bhat <- object$coefficients
  
  # Compute η and λ
  eta <- as.vector(X %*% bhat)
  lambda <- exp(eta)
  
  # Extract scalar μ from object (adjust this line as needed)
  mu <-  object$mu 
  
  # Compute expected response: E[Y] = λ * μ
  mean_response <- lambda * mu
  
  # Delta method variance: d/dβ [exp(η) * μ] = μ * exp(η) * X
  grad <- diag(as.vector(mu * exp(eta))) %*% X
  
  # Variance of β
  V_beta <- vcov(object)
  V_emm <- grad %*% V_beta %*% t(grad)
  
  # Dummy bhat and X (not used due to postGridHook)
  dummy_bhat <- rep(0, ncol(X))
  dummy_X <- matrix(0, nrow = nrow(X), ncol = ncol(X))
  colnames(dummy_X) <- colnames(X)
  
  # Degrees of freedom
  dfargs <- list(df = if (!is.null(object$df_residual)) object$df_residual else Inf)
  dffun <- function(k, dfargs) dfargs$df
  
  list(
    X = dummy_X,
    bhat = dummy_bhat,
    V = V_emm,
    nbasis = matrix(0, 0, length(dummy_bhat)),
    dffun = dffun,
    dfargs = dfargs,
    misc = list(
      tran = "identity",
      estName = "E[Y]",
      inv.link = identity
    ),
    postGridHook = function(grid) {
      grid$emmean <- mean_response
      grid
    }
  )
}

# 
# summary.cpn <- function(object, ...) {
#   # Separate mu and sigma from coefficients
#   beta_hat <- object$coefficients
#   se_hat <- object$se[1:length(beta_hat)]
#   
#   # z-values and p-values for coefficients only
#   z_vals <- beta_hat / se_hat
#   p_vals <- 2 * pnorm(-abs(z_vals))
#   
#   param_names <- names(beta_hat)
#   
#   summary_table <- data.frame(
#     Estimate = beta_hat,
#     Std.Error = se_hat,
#     z.value = z_vals,
#     Pr.z = p_vals,
#     row.names = param_names
#   )
#   
#   # Deviance residuals summary
#   dev_res <- object$deviance_residuals
#   dev_summary <- quantile(dev_res, probs = c(0, 0.25, 0.5, 0.75, 1))
#   names(dev_summary) <- c("Min", "1Q", "Median", "3Q", "Max")
#   
#   # Only print output if this is the top-level call (e.g., user typed summary())
#   if (sys.nframe() <= 1 || interactive()) {
#     cat("Call:\n")
#     print(object$call)
#     
#     cat("\nDeviance Residuals:\n")
#     print(dev_summary, digits = 4, quote = FALSE)
#     
#     cat("\nCoefficients:\n")
#     printCoefmat(summary_table, P.values = TRUE, has.Pvalue = TRUE)
#     
#     # Print mu and sigma separately
#     cat(sprintf("\nEstimated mu parameter: %.4f\n", object$mu))
#     cat(sprintf("Estimated sigma parameter: %.4f\n", object$sigma))
#     
#     # Deviance summary
#     cat(sprintf("\nNull deviance: %.2f on %d degrees of freedom\n",
#                 object$null_deviance, object$df_null))
#     cat(sprintf("Residual deviance: %.2f on %d degrees of freedom\n",
#                 object$residual_deviance, object$df_residual))
#     cat(sprintf("AIC: %.2f\n", object$aic))
#   }
#   
#   invisible(summary_table)
# }

lm_fit <- lm(y~group, data=df_sim)
names(summary(lm_fit))

summary(lm_fit)$coefficients

anova(lm_fit)



cpn <- function(formula, data = NULL, mu_init = NULL, sigma_init = NULL, epsilon = 1e-6, Kmax=10) {
  
  if (Kmax<10 & Kmax >100) stop("Kmax should be between 10 and 100")
  
  
  # Ensure required package
  if (!requireNamespace("numDeriv", quietly = TRUE)) {
    stop("Package 'numDeriv' is required but not installed.")
  }
  
  
  # Build model frame and design matrix (allow formula to work with or without data argument)
  formula <- stats::as.formula(formula, env = parent.frame())
  mf <- stats::model.frame(formula = formula, data = data)
  y <- model.response(mf)
  X <- model.matrix(attr(mf, "terms"), data = mf)
  
  # Initial values
  y_nonzero <- y
  if (is.null(mu_init)) mu_init <- mean(y_nonzero)
  if (is.null(sigma_init)) sigma_init <- sd(y_nonzero)
  init_beta <- rep(0, ncol(X))
  init_vals <- c(init_beta, mu_init, sigma_init)
  
  # Optimize negative log-likelihood
  fit <- optim(
    par = init_vals,
    fn = cpn_regression_neg_log_likelihood,
    X = X,
    y = y,
    Kmax = Kmax,
    method = "Nelder-Mead",
    control = list(maxit = 1000)
  )
  
  beta_hat <- fit$par
  loglik <- -fit$value
  
  # Compute Hessian and standard errors
  # H <- numDeriv::hessian(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  #H <- hessian_fd(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  
  H <- tryCatch({
    numDeriv::hessian(func = function(par) cpn_regression_neg_log_likelihood(par, X, y, Kmax = Kmax),
                      x = beta_hat)
  }, error = function(e) {
    warning("Failed to compute Hessian: ", e$message)
    return(matrix(NA, length(beta_hat), length(beta_hat)))
  })
  
  # H <- hessian_fd_nosym(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  if (any(is.na(H)) || det(H) == 0 || any(!is.finite(H))) {
    warning("Hessian is singular or contains non-finite values; SEs are not available.")
    se_hat <- rep(NA, length(beta_hat))
  } else {
    eigs <- eigen(H, symmetric = TRUE)$values
    if (any(eigs <= 0)) {
      warning("Hessian is not positive definite; SEs may be unreliable.")
      se_hat <- sqrt(diag(solve(H)))
    } else {
      se_hat <- sqrt(diag(solve(H)))
    }
  }
  
  
  param_names <- c(colnames(X), "mu", "sigma")
  names(beta_hat) <- param_names
  names(se_hat) <- param_names
  
  # # Print parameter estimates with SEs
  # cat("Parameter estimates with standard errors:\n")
  # print(data.frame(Estimate = beta_hat, SE = se_hat, row.names = param_names))
  
  # Compute fitted values (linear predictor)
  eta <- as.vector(X %*% beta_hat[1:ncol(X)])
  lambda_hat <- exp(eta)  # Poisson mean
  mu_hat <- beta_hat["mu"]
  sigma_hat <- beta_hat["sigma"]
  
  # Compute individual log-likelihood contributions
  loglik_obs <- function(y_i, lambda_i) {
    if (y_i == 0) {
      log_p0 <- dpois(0, lambda_i)
      return(log(log_p0))
    } else {
      # Convolution of Poisson and Normal (approximate): use numerical integration or approximation
      # We'll use simplified approximation for residuals:
      expected <- lambda_i * mu_hat
      var <- lambda_i * sigma_hat^2
      return(dnorm(y_i, mean = expected, sd = sqrt(var), log = TRUE))
    }
  }
  
  loglik_saturated <- function(y_i) {
    # In saturated model, y_i is predicted perfectly → log-lik is 0 if Normal
    # For residuals, we assume perfect fit implies maximum likelihood → return 0
    return(0)
  }
  
  dev_res <- numeric(length(y))
  for (i in seq_along(y)) {
    ll_hat <- loglik_obs(y[i], lambda_hat[i])
    ll_sat <- loglik_saturated(y[i])
    dev_res[i] <- sign(y[i] - lambda_hat[i] * mu_hat) * sqrt(2 * (ll_sat - ll_hat))
  }
  
  # Null model (intercept only)
  X_null <- matrix(1, nrow = nrow(X), ncol = 1)
  colnames(X_null) <- "(Intercept)"
  null_fit <- optim(
    par = c(0, mu_init, sigma_init),
    fn = cpn_regression_neg_log_likelihood,
    X = X_null,
    y = y,
    method = "Nelder-Mead",
    Kmax = Kmax,
    control = list(maxit = 1000)
  )
  null_loglik <- -null_fit$value
  
  null_deviance <- -2 * null_loglik
  residual_deviance <- -2 * loglik
  df_null <- length(y) - 1
  df_residual <- length(y) - length(beta_hat)
  
  # AIC: -2 * log-likelihood + 2 * number of parameters
  k <- length(beta_hat)
  aic_val <- 2 * k + 2 * fit$value  # equivalent to -2 * loglik + 2k
  
  model_frame <- model.frame(formula, data)
  terms_obj <- terms(model_frame)
  
  # Add to returned list
  structure(list(
    coefficients = setNames(beta_hat[1:ncol(X)], colnames(X)),
    mu = beta_hat["mu"],
    sigma = beta_hat["sigma"],
    se = se_hat,
    model = model_frame,
    terms = terms_obj,
    formula = formula,
    data = data,
    deviance_residuals = dev_res,
    fitted_values = lambda_hat * mu_hat,
    neg_log_likelihood = fit$value,
    null_deviance = null_deviance,
    residual_deviance = residual_deviance,
    df_null = df_null,
    df_residual = df_residual,
    aic = aic_val,
    call = match.call()
  ), class = "cpn")
}


anova.cpn <- function(object, ...) {
  # Extract components
  full_formula <- formula(object)
  terms_obj <- terms(object)
  term_labels <- attr(terms_obj, "term.labels")
  
  if (length(term_labels) == 0) {
    warning("Model contains only an intercept. No terms to analyze.")
    return(invisible(NULL))
  }
  
  response <- as.character(attr(terms_obj, "variables"))[2]
  data <- object$data
  deviance_vals <- c()
  df_vals <- c()
  models <- list()
  
  # Fit null (intercept-only) model
  current_formula <- as.formula(paste(response, "~ 1"))
  null_model <- cpn(current_formula, data = data)
  current_dev <- null_model$residual_deviance
  current_df <- null_model$df_residual
  deviance_vals <- c(deviance_vals, current_dev)
  df_vals <- c(df_vals, current_df)
  
  # Sequentially add each term
  for (i in seq_along(term_labels)) {
    term_subset <- paste(term_labels[1:i], collapse = " + ")
    formula_i <- as.formula(paste(response, "~", term_subset))
    model_i <- cpn(formula_i, data = data)
    deviance_vals <- c(deviance_vals, model_i$residual_deviance)
    df_vals <- c(df_vals, model_i$df_residual)
    models[[i]] <- model_i
  }
  
  # Calculate sequential deviance differences
  dev_diff <- c(NA, diff(deviance_vals))
  df_diff <- c(NA, diff(df_vals))
  p_vals <- c(NA, pchisq(dev_diff[-1], df = df_diff[-1], lower.tail = FALSE))
  
  anova_table <- data.frame(
    Step = c("(Intercept)", term_labels),
    Df = df_diff,
    Deviance = dev_diff,
    Resid.Df = df_vals,
    Resid.Dev = deviance_vals,
    `Pr(>Chi)` = c(NA, p_vals),
    row.names = NULL
  )
  
  class(anova_table) <- c("anova", "data.frame")
  return(anova_table)
}

anova(cpn_fit)

#' anova.cpn <- function(object, ...) {
#'   full_formula <- formula(object)
#'   terms_obj <- terms(object)
#'   term_labels <- attr(terms_obj, "term.labels")
#'   
#'   if (length(term_labels) == 0) {
#'     warning("Model contains only an intercept. No terms to test.")
#'     return(invisible(NULL))
#'   }
#'   
#'   response <- as.character(attr(terms_obj, "variables"))[2]
#'   data <- object$data
#'   
#'   # Start with intercept-only model
#'   null_formula <- as.formula(paste(response, "~ 1"))
#'   fit_prev <- cpn(null_formula, data = data)
#'   dev_prev <- fit_prev$residual_deviance
#'   df_prev <- fit_prev$df_residual
#'   
#'   rows <- list()
#'   rows[[1]] <- list(Term = "(Intercept)", Df = NA, Deviance = NA,
#'                     `Resid. Df` = df_prev, `Resid. Dev` = dev_prev, `Pr(>Chi)` = NA)
#'   
#'   for (i in seq_along(term_labels)) {
#'     term <- term_labels[i]
#'     current_formula <- as.formula(paste(response, "~", paste(term_labels[1:i], collapse = " + ")))
#'     fit_curr <- cpn(current_formula, data = data)
#'     
#'     dev_curr <- fit_curr$residual_deviance
#'     df_curr <- fit_curr$df_residual
#'     
#'     df_diff <- df_prev - df_curr
#'     dev_diff <- dev_prev - dev_curr
#'     p_val <- if (!is.na(df_diff) && df_diff > 0) {
#'       pchisq(dev_diff, df = df_diff, lower.tail = FALSE)
#'     } else {
#'       NA
#'     }
#'     
#'     rows[[i + 1]] <- list(Term = term, Df = df_diff, Deviance = dev_diff,
#'                           `Resid. Df` = df_curr, `Resid. Dev` = dev_curr, `Pr(>Chi)` = p_val)
#'     
#'     dev_prev <- dev_curr
#'     df_prev <- df_curr
#'   }
#'   
#'   anova_df <- do.call(rbind, lapply(rows, as.data.frame))
#'   rownames(anova_df) <- NULL
#'   class(anova_df) <- c("anova", "data.frame")
#'   return(anova_df)
#' }
#' 
#' #' @export
#' print.anova.cpn <- function(x, digits = max(4, getOption("digits") - 2), ...) {
#'   signif_stars <- symnum(x$`Pr(>Chi)`,
#'                          corr = FALSE, na = FALSE,
#'                          cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
#'                          symbols = c("***", "**", "*", ".", " "))
#'   
#'   out <- cbind(x, Signif = signif_stars)
#'   print.data.frame(out, digits = digits, row.names = FALSE, ...)
#'   cat("---\n")
#'   cat("Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n")
#'   invisible(x)
#' }


anova.cpn <- function(object, ..., test = "Chisq") {
  more_models <- list(...)
  
  # If additional models are provided: compare multiple CPN fits
  if (length(more_models) > 0) {
    models <- c(list(object), more_models)
    
    # Check all inputs are of class "cpn"
    if (!all(sapply(models, inherits, "cpn"))) {
      stop("All inputs must be of class 'cpn'")
    }
    
    # Extract and order by residual df (increasing model complexity)
    df_res <- sapply(models, function(m) m$df_residual)
    dev <- sapply(models, function(m) m$residual_deviance)
    n_models <- length(models)
    
    ord <- order(df_res)
    models <- models[ord]
    df_res <- df_res[ord]
    dev <- dev[ord]
    
    # Compute differences and p-values
    df_diff <- c(NA, diff(df_res))
    dev_diff <- c(NA, diff(dev))
    p_vals <- c(NA, pchisq(dev_diff[-1], df_diff[-1], lower.tail = FALSE))
    
    # Build comparison table
    result <- data.frame(
      Model = paste0("Model ", seq_len(n_models)),
      Residual_Df = df_res,
      Residual_Deviance = dev,
      Df = df_diff,
      Deviance = dev_diff,
      `Pr(>Chi)` = p_vals,
      check.names = FALSE
    )
    
    class(result) <- c("anova", "data.frame")
    return(result)
    
  } else {
    # Single-model mode: sequential deviance table by term
    full_formula <- formula(object)
    terms_obj <- terms(object)
    term_labels <- attr(terms_obj, "term.labels")
    
    if (length(term_labels) == 0) {
      warning("Model contains only an intercept. No terms to test.")
      return(invisible(NULL))
    }
    
    response <- as.character(attr(terms_obj, "variables"))[2]
    data <- object$data
    
    # Intercept-only model
    null_formula <- as.formula(paste(response, "~ 1"))
    fit_prev <- cpn(null_formula, data = data)
    dev_prev <- fit_prev$residual_deviance
    df_prev <- fit_prev$df_residual
    
    rows <- list()
    rows[[1]] <- list(Term = "(Intercept)", Df = NA, Deviance = NA,
                      `Resid. Df` = df_prev, `Resid. Dev` = dev_prev, `Pr(>Chi)` = NA)
    
    for (i in seq_along(term_labels)) {
      term <- term_labels[i]
      current_formula <- as.formula(paste(response, "~", paste(term_labels[1:i], collapse = " + ")))
      fit_curr <- cpn(current_formula, data = data)
      
      dev_curr <- fit_curr$residual_deviance
      df_curr <- fit_curr$df_residual
      
      df_diff <- df_prev - df_curr
      dev_diff <- dev_prev - dev_curr
      p_val <- if (!is.na(df_diff) && df_diff > 0) {
        pchisq(dev_diff, df = df_diff, lower.tail = FALSE)
      } else {
        NA
      }
      
      rows[[i + 1]] <- list(Term = term, Df = df_diff, Deviance = dev_diff,
                            `Resid. Df` = df_curr, `Resid. Dev` = dev_curr, `Pr(>Chi)` = p_val)
      
      dev_prev <- dev_curr
      df_prev <- df_curr
    }
    
    anova_df <- do.call(rbind, lapply(rows, as.data.frame))
    rownames(anova_df) <- NULL
    class(anova_df) <- c("anova", "data.frame")
    return(anova_df)
  }
}

# Optional: Fancy print with significance stars
print.anova.cpn <- function(x, digits = max(4, getOption("digits") - 2), ...) {
  if (!("Term" %in% names(x))) {
    print.data.frame(x, digits = digits, ...)
  } else {
    signif_stars <- symnum(x$`Pr(>Chi)`,
                           corr = FALSE, na = FALSE,
                           cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                           symbols = c("***", "**", "*", ".", " "))
    out <- cbind(x, Signif = signif_stars)
    print.data.frame(out, digits = digits, row.names = FALSE, ...)
    cat("---\n")
    cat("Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n")
  }
  invisible(x)
}

anova(m1, m2)
anova(cpn_fit)
