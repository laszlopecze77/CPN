library(ggplot2)
#library(numDeriv)

###############################
# Compound Poisson Normal MLE #
###############################
find_Kmax <- function(lambda, epsilon = 1e-6) {
  Kmax <- 0
  tail_prob <- 1 - ppois(Kmax, lambda)
  
  while (tail_prob > epsilon) {
    Kmax <- Kmax + 1
    tail_prob <- 1 - ppois(Kmax, lambda)
  }
  
  return(Kmax)
}

# Function to compute Hessian using central differences
hessian_fd <- function(func, x, ..., eps = 1e-5) {
  n <- length(x)
  H <- matrix(0, n, n)
  fx <- func(x, ...)
  
  for (i in 1:n) {
    for (j in i:n) {
      x_ij_pp <- x_ij_pm <- x_ij_mp <- x_ij_mm <- x
      
      x_ij_pp[i] <- x_ij_pp[i] + eps
      x_ij_pp[j] <- x_ij_pp[j] + eps
      
      x_ij_pm[i] <- x_ij_pm[i] + eps
      x_ij_pm[j] <- x_ij_pm[j] - eps
      
      x_ij_mp[i] <- x_ij_mp[i] - eps
      x_ij_mp[j] <- x_ij_mp[j] + eps
      
      x_ij_mm[i] <- x_ij_mm[i] - eps
      x_ij_mm[j] <- x_ij_mm[j] - eps
      
      H[i, j] <- (func(x_ij_pp, ...) - func(x_ij_pm, ...) - 
                    func(x_ij_mp, ...) + func(x_ij_mm, ...)) / (4 * eps^2)
      
      if (i != j) {
        H[j, i] <- H[i, j]  # exploit symmetry
      }
    }
  }
  
  return(H)
}

# hessian_fd_nosym <- function(func, x, ..., eps = 1e-5) {
#   n <- length(x)
#   H <- matrix(0, n, n)
#   
#   for (i in 1:n) {
#     for (j in 1:n) {
#       x_ij_pp <- x_ij_pm <- x_ij_mp <- x_ij_mm <- x
#       
#       x_ij_pp[i] <- x_ij_pp[i] + eps
#       x_ij_pp[j] <- x_ij_pp[j] + eps
#       
#       x_ij_pm[i] <- x_ij_pm[i] + eps
#       x_ij_pm[j] <- x_ij_pm[j] - eps
#       
#       x_ij_mp[i] <- x_ij_mp[i] - eps
#       x_ij_mp[j] <- x_ij_mp[j] + eps
#       
#       x_ij_mm[i] <- x_ij_mm[i] - eps
#       x_ij_mm[j] <- x_ij_mm[j] - eps
#       
#       H[i, j] <- (func(x_ij_pp, ...) - func(x_ij_pm, ...) -
#                     func(x_ij_mp, ...) + func(x_ij_mm, ...)) / (4 * eps^2)
#     }
#   }
#   
#   return(H)
# }

cpn_regression_neg_log_likelihood <- function(beta_mu_sigma, X, y) {
  
  # Extract parameters
  p <- ncol(X)
  beta <- beta_mu_sigma[1:p]        # Regression coefficients for lambda
  mu <- beta_mu_sigma[p + 1]        # Mean of normal components
  sigma <- beta_mu_sigma[p + 2]     # Std dev of normal components
  
  if (sigma <= 0) return(Inf)       # Penalize invalid sigma
  
  # Linear predictor for lambda (log-link)
  eta <- X %*% beta
  lambda_vec <- as.vector(exp(eta))  # Observation-specific lambda
  
  # Determine a maximum K across all lambda (for truncation)
  Kmax <- find_Kmax(max(lambda_vec))
  
  # Compute likelihood for each observation
  likelihoods <- mapply(function(x_i, lambda_i) {
    
    if (x_i == 0) {
      prob <- dpois(0, lambda_i)
    } else {
      k_vals <- 1:Kmax
      poisson_probs <- dpois(k_vals, lambda_i)
      normal_probs <- dnorm(x_i, mean = k_vals * mu, sd = sqrt(k_vals * sigma^2))
      prob <- sum(poisson_probs * normal_probs)
    }
    
    return(prob)
    
  }, y, lambda_vec)
  
  # If any likelihood is 0 (log(0) = -Inf), return Inf for the negative log-likelihood
  if (any(likelihoods <= 0)) return(Inf)
  
  return(-sum(log(likelihoods)))
}


cpn_reg_mle <- function(X, y) {
  # X: Design matrix (n x p)
  # y: Observed response vector (length n)
  
  n <- length(y)
  p <- ncol(X)
  
  # Initial values:
  init_beta <- rep(0, p)              # Start with 0 for all regression coefficients
  init_mu <- mean(y[y > 0])           # Mean of non-zero observations as proxy for mu
  init_sigma <- sd(y[y > 0])          # Std dev of non-zero observations as proxy for sigma
  
  init_vals <- c(init_beta, init_mu, init_sigma)  # Concatenate all initial values
  
  # Optimization
  mle_result <- optim(
    par = init_vals,
    fn = cpn_regression_neg_log_likelihood,
    X = X,
    y = y,
    method = "Nelder-Mead",
    control = list(maxit = 1000)  # You can tweak this for better convergence
  )
  
  return(mle_result$par)  # Returns c(beta_hat, mu_hat, sigma_hat)
}


cpn <- function(formula, data, mu_init = NULL, sigma_init = NULL, epsilon = 1e-6) {
  # # Ensure required package
  # if (!requireNamespace("numDeriv", quietly = TRUE)) {
  #   stop("Package 'numDeriv' is required but not installed.")
  # }
  
  # Build model frame and design matrix
  mf <- model.frame(formula, data)
  y <- model.response(mf)
  X <- model.matrix(attr(mf, "terms"), data = mf)
  
  # Initial values
  y_nonzero <- y[y > 0]
  if (is.null(mu_init)) mu_init <- mean(y_nonzero)
  if (is.null(sigma_init)) sigma_init <- sd(y_nonzero)
  init_beta <- rep(0, ncol(X))
  init_vals <- c(init_beta, mu_init, sigma_init)
  
  # Optimize negative log-likelihood
  fit <- optim(
    par = init_vals,
    fn = cpn_regression_neg_log_likelihood,
    X = X,
    y = y,
    method = "Nelder-Mead",
    control = list(maxit = 1000)
  )
  
  beta_hat <- fit$par
  loglik <- -fit$value
  
  # Compute Hessian and standard errors
 # H <- numDeriv::hessian(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  H <- hessian_fd(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
 # H <- hessian_fd_nosym(func = cpn_regression_neg_log_likelihood, x = beta_hat, X = X, y = y)
  if (any(is.na(H)) || det(H) == 0 || any(!is.finite(H))) {
    warning("Hessian is singular or contains non-finite values; SEs are not available.")
    se_hat <- rep(NA, length(beta_hat))
  } else {
    se_hat <- sqrt(diag(solve(H)))
  }
  

  param_names <- c(colnames(X), "mu", "sigma")
  names(beta_hat) <- param_names
  names(se_hat) <- param_names
  
  # # Print parameter estimates with SEs
  # cat("Parameter estimates with standard errors:\n")
  # print(data.frame(Estimate = beta_hat, SE = se_hat, row.names = param_names))
  
  # Compute fitted values (linear predictor)
  eta <- as.vector(X %*% beta_hat[1:ncol(X)])
  lambda_hat <- exp(eta)  # Poisson mean
  mu_hat <- beta_hat["mu"]
  sigma_hat <- beta_hat["sigma"]
  
  # Compute individual log-likelihood contributions
  loglik_obs <- function(y_i, lambda_i) {
    if (y_i == 0) {
      log_p0 <- dpois(0, lambda_i)
      return(log(log_p0))
    } else {
      # Convolution of Poisson and Normal (approximate): use numerical integration or approximation
      # We'll use simplified approximation for residuals:
      expected <- lambda_i * mu_hat
      var <- lambda_i * sigma_hat^2
      return(dnorm(y_i, mean = expected, sd = sqrt(var), log = TRUE))
    }
  }
  
  loglik_saturated <- function(y_i) {
    # In saturated model, y_i is predicted perfectly → log-lik is 0 if Normal
    # For residuals, we assume perfect fit implies maximum likelihood → return 0
    return(0)
  }
  
  dev_res <- numeric(length(y))
  for (i in seq_along(y)) {
    ll_hat <- loglik_obs(y[i], lambda_hat[i])
    ll_sat <- loglik_saturated(y[i])
    dev_res[i] <- sign(y[i] - lambda_hat[i] * mu_hat) * sqrt(2 * (ll_sat - ll_hat))
  }
  
  # Null model (intercept only)
  X_null <- matrix(1, nrow = nrow(X), ncol = 1)
  colnames(X_null) <- "(Intercept)"
  null_fit <- optim(
    par = c(0, mu_init, sigma_init),
    fn = cpn_regression_neg_log_likelihood,
    X = X_null,
    y = y,
    method = "Nelder-Mead",
    control = list(maxit = 1000)
  )
  null_loglik <- -null_fit$value
  
  null_deviance <- -2 * null_loglik
  residual_deviance <- -2 * loglik
  df_null <- length(y) - 1
  df_residual <- length(y) - length(beta_hat)
  
  # AIC: -2 * log-likelihood + 2 * number of parameters
  k <- length(beta_hat)
  aic_val <- 2 * k + 2 * fit$value  # equivalent to -2 * loglik + 2k
  
  # Add to returned list
  structure(list(
    coefficients = setNames(beta_hat[1:ncol(X)], colnames(X)),
    mu = beta_hat["mu"],
    sigma = beta_hat["sigma"],
    se = se_hat,
    formula = formula,
    data = data,
    deviance_residuals = dev_res,
    fitted_values = lambda_hat * mu_hat,
    neg_log_likelihood = fit$value,
    null_deviance = null_deviance,
    residual_deviance = residual_deviance,
    df_null = df_null,
    df_residual = df_residual,
    aic = aic_val,
    call = match.call()
  ), class = "cpn")
}

summary.cpn <- function(object, ...) {
  # Separate mu and sigma from coefficients
  beta_hat <- object$coefficients
  se_hat <- object$se[1:length(beta_hat)]
  
  # z-values and p-values for coefficients only
  z_vals <- beta_hat / se_hat
  p_vals <- 2 * pnorm(-abs(z_vals))
  
  param_names <- names(beta_hat)
  
  summary_table <- data.frame(
    Estimate = beta_hat,
    Std.Error = se_hat,
    z.value = z_vals,
    Pr.z = p_vals,
    row.names = param_names
  )
  
  # Deviance residuals summary
  dev_res <- object$deviance_residuals
  dev_summary <- quantile(dev_res, probs = c(0, 0.25, 0.5, 0.75, 1))
  names(dev_summary) <- c("Min", "1Q", "Median", "3Q", "Max")
  
  cat("Call:\n")
  print(object$call)
  
  cat("\nDeviance Residuals:\n")
  print(dev_summary, digits = 4, quote = FALSE)
  
  cat("\nCoefficients:\n")
  printCoefmat(summary_table, P.values = TRUE, has.Pvalue = TRUE)
  
  # Print mu and sigma separately
  cat(sprintf("\nEstimated mu parameter: %.4f\n", object$mu))
  cat(sprintf("Estimated sigma parameter: %.4f\n", object$sigma))
  
  # Deviance summary
  cat(sprintf("\nNull deviance: %.2f on %d degrees of freedom\n",
              object$null_deviance, object$df_null))
  cat(sprintf("Residual deviance: %.2f on %d degrees of freedom\n",
              object$residual_deviance, object$df_residual))
  cat(sprintf("AIC: %.2f\n", object$aic))
  
  invisible(summary_table)
}

plot.cpn <- function(x, ...) {
  if (is.null(x$deviance_residuals) || is.null(x$fitted_values)) {
    stop("Model must include deviance_residuals and fitted_values for plotting.")
  }
  
  par(mfrow = c(2, 2))
  
  # 1. Residuals vs Fitted
  plot(x$fitted_values, x$deviance_residuals,
       xlab = "Fitted values", ylab = "Deviance residuals",
       main = "Residuals vs Fitted")
  abline(h = 0, col = "red", lty = 2)
  
  # 2. Q-Q Plot
  qqnorm(x$deviance_residuals, main = "Normal Q-Q")
  qqline(x$deviance_residuals, col = "red")
  
  # 3. Histogram
  hist(x$deviance_residuals, breaks = 20, main = "Histogram of Residuals",
       xlab = "Deviance residuals", col = "gray")
  
  # 4. Scale-Location (sqrt(|resid|) vs fitted)
  sqrt_abs_resid <- sqrt(abs(x$deviance_residuals))
  plot(x$fitted_values, sqrt_abs_resid,
       xlab = "Fitted values", ylab = expression(sqrt("|Deviance residuals|")),
       main = "Scale-Location")
  abline(h = median(sqrt_abs_resid), col = "red", lty = 2)
  
  par(mfrow = c(1, 1))
}

plot_observed_vs_fitted.cpn <- function(model, ...) {
  if (is.null(model$fitted_values)) {
    stop("Model does not contain fitted values.")
  }
  
  y <- model.response(model.frame(model$formula, data = model$data))
  
  # Save only modifiable graphical parameters
  old_mar <- par("mar")
  old_mfrow <- par("mfrow")
  on.exit({
    par(mar = old_mar)
    par(mfrow = old_mfrow)
  })
  
  # Set default margins and layout
  par(mar = c(5, 5, 4, 2) + 0.1)
  par(mfrow = c(1, 1))
  
  plot(model$fitted_values, y,
       xlab = "Fitted values",
       ylab = "Observed values",
       main = "Observed vs Fitted Values",
       pch = 19, col = "darkgreen")
  abline(a = 0, b = 1, col = "blue", lty = 2)
}

anova.cpn <- function(object, ..., test = "Chisq") {
  # Collect all models
  models <- list(object, ...)
  n_models <- length(models)
  
  # Check all are of class 'cpn'
  if (!all(sapply(models, inherits, "cpn"))) {
    stop("All objects must be of class 'cpn'")
  }
  
  # Extract model stats
  df_res <- sapply(models, function(m) m$df_residual)
  dev <- sapply(models, function(m) m$residual_deviance)
  n <- length(models[[1]]$data[[1]])
  
  # Sort by model complexity (smaller df_res => more complex)
  ord <- order(df_res)
  models <- models[ord]
  df_res <- df_res[ord]
  dev <- dev[ord]
  
  # Compute differences
  df_diff <- c(NA, diff(df_res))
  dev_diff <- c(NA, diff(dev))
  
  # p-values for Chi-squared test
  p_vals <- c(NA, pchisq(dev_diff[-1], df_diff[-1], lower.tail = FALSE))
  
  # Build result table
  result <- data.frame(
    Residual_Df = df_res,
    Residual_Deviance = dev,
    Df = df_diff,
    Deviance = dev_diff,
    Pr_Chisq = c(NA, p_vals)
  )
  rownames(result) <- paste0("Model ", seq_len(n_models))
  
  class(result) <- c("anova", "data.frame")
  return(result)
}

###############
# Test model  #
###############

simulate_cpn_data <- function(n = 200, beta = c(0.5, -0.3, 0.7), mu = 2, sigma = 1) {
  set.seed(123)  # For reproducibility
  
  # Simulate predictors
  x1 <- factor(sample(c("A", "B"), size = n, replace = TRUE))  # Categorical
  x2 <- rnorm(n)  # Continuous
  
  # Create model matrix (includes intercept and dummy variable for x1)
  X <- model.matrix(~ x1 + x2)  # Will generate intercept, x1B, x2
  
  # Compute linear predictor and Poisson rates
  eta <- X %*% beta
  lambda <- exp(eta)
  
  # Simulate response variable
  y <- numeric(n)
  for (i in 1:n) {
    k <- rpois(1, lambda[i])
    if (k > 0) {
      y[i] <- sum(rnorm(k, mean = mu, sd = sigma))
    } else {
      y[i] <- 0
    }
  }
  
  # Return as a data.frame
  data.frame(y = y, x1 = x1, x2 = x2)
}

df2 <- simulate_cpn_data()
head(df2)


fit <- cpn(y ~ x1 + x2, data = df2)

fit$fitted_values

summary(fit)

plot_observed_vs_fitted.cpn(fit)

m1 <- cpn(y ~ 1, data = df2)
summary(m1)
m2 <- cpn(y ~ x1 + x2, data = df2)
summary(m2)

anova(m1, m2)



simulate_cpn <- function(n, lambda, mu, sigma) {
  counts <- rpois(n, lambda)  # Draw the number of events (k) for each of the n observations from a Poisson distribution
  data <- sapply(counts, function(k) sum(rnorm(k, mean = mu, sd = sigma)))  # For each k, simulate k independent normal variables and sum them
  return(data)  # Return the vector of compound outcomes
}

set.seed(123)  # Set seed for reproducibility
sim_data1 <- simulate_cpn(n = 200, lambda = 1.3, mu = 2, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution
sim_data2 <- simulate_cpn(n = 100, lambda = 1, mu = 2, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution

# Combine data and group labels
sim_data_pooled <- c(sim_data1, sim_data2)
group <- factor(c(rep(1, length(sim_data1)), rep(2, length(sim_data2))))

df1 <- data.frame(res=sim_data_pooled,
                  group=group)

fit <- cpn(res ~ group, data = df1)

fit$fitted_values

summary(fit)


plot(fit)

plot_observed_vs_fitted.cpn(fit)


