
library(ggplot2)

###############################
# Compound Poisson Normal MLE #
###############################
find_Kmax <- function(lambda, epsilon = 1e-6) {
  Kmax <- 0
  tail_prob <- 1 - ppois(Kmax, lambda)
  
  while (tail_prob > epsilon) {
    Kmax <- Kmax + 1
    tail_prob <- 1 - ppois(Kmax, lambda)
  }
  
  return(Kmax)
}


# Function to simulate a Compound Poisson-Normal dataset
simulate_cpn <- function(n, lambda, mu, sigma) {
  counts <- rpois(n, lambda)  # Draw the number of events (k) for each of the n observations from a Poisson distribution
  data <- sapply(counts, function(k) sum(rnorm(k, mean = mu, sd = sigma)))  # For each k, simulate k independent normal variables and sum them
  return(data)  # Return the vector of compound outcomes
}

sim_cpn <- simulate_cpn(100000, 1.2, 7, 20)

mean(sim_cpn)
1.2 * 7

var(sim_cpn)
1.2 * (20 *20 + 7*7)



# Plot histogram of simulated data
hist(simulate_cpn(1000, 1.5, 7, 10), breaks = 30)  

# Negative log-likelihood function for Compound Poisson-Normal distribution
cpn_neg_log_likelihood <- function(params, data) {
  lambda <-  params[1]  # Poisson rate (expected number of normal components per observation)
  mu <-   params[2]     # Mean of each normal component
  sigma <-   params[3]  # Standard deviation of each normal component
  
  if (lambda <= 0 || sigma <= 0) return(Inf)  # Return infinite likelihood if invalid parameter values are proposed (to penalize optimizer)
  
  Kmax <- find_Kmax(lambda) # Maximum number of Poisson events to consider in the summation (truncate the infinite Poisson sum for computational feasibility)
  
  
  # Compute the likelihood for each observation in the data
  likelihoods <- sapply(data, function(x) {
    
    if (x == 0) {
      prob <- dpois(0, lambda)  # Special case: if the observed sum is 0, assume it came from 0 events (k=0)
    } else {
      k_vals <- 1:Kmax  # Consider k = 1 to Kmax
      poisson_probs <- dpois(k_vals, lambda)  # Probability of k events occurring (Poisson)
      normal_probs <- dnorm(x, mean = k_vals * mu, sd = sqrt(k_vals * sigma^2))  # Probability of observing x given k normal components
      prob <-  sum(poisson_probs * normal_probs)  # Marginal probability of x via convolution (weighted sum over possible k)
    }
    
    return(prob)  # Return the marginal probability for each observation
  })
  
  return(-sum(log(likelihoods)))  # Return the total negative log-likelihood to be minimized (MLE objective)
}



# Maximum Likelihood Estimation function for the Compound Poisson-Normal model
# Goal: Minimize a nonlinear function f(theta) where theta is a vector of parameters.
# BFGS is gradient-based, meaning it uses first derivatives (the gradient) of the function to move toward a minimum.
# Unlike Newton's method (which uses second derivatives — the Hessian), BFGS approximates the Hessian iteratively using only gradients. 
# Nelder-Mead (Simplex Algorithm) - Doesn’t use derivatives — just evaluates the function at different points.

cpn_mle <- function(data) {
  init_vals <- c(1, mean(data), sd(data))  # Provide starting values based on simple moment-based estimates
  mle_result <- optim(
    par = init_vals,  # Initial guesses for lambda, mu, sigma
    fn = cpn_neg_log_likelihood,  # Objective function to minimize
    data = data,  # Observed data
    method = "Nelder-Mead"  # Optimization algorithm
  )
  return(mle_result$par)  # Return the estimated parameter values (lambda, mu, sigma)
}



# Simulate a dataset and estimate the model parameters
set.seed(123)  # Set seed for reproducibility
sim_data <- simulate_cpn(n = 1000, lambda = 0.7, mu = -1000, sigma = 800)  # Generate 1000 observations from a Compound Poisson-Normal distribution
mean(sim_data)  # Compute and print the sample mean
hist(sim_data, breaks = 30)  # Plot histogram of simulated data

mle_estimates <- cpn_mle(sim_data)  # Estimate lambda, mu, and sigma using MLE


cat("Estimated lambda:", mle_estimates[1], "\n")
cat("Estimated mu:", mle_estimates[2], "\n")
cat("Estimated sigma:", mle_estimates[3], "\n")
cat("Estimated mean:", mle_estimates[1] * mle_estimates[2], "\n")

mean(sim_data)

# Extract MLEs
lambda_mle <- mle_estimates[1]
mu_mle <- mle_estimates[2]
sigma_mle <- mle_estimates[3]

# Compute maximum log-likelihood at MLE
logLik_mle <- -cpn_neg_log_likelihood(c(lambda_mle, mu_mle, sigma_mle), sim_data)
logLik_mle

#######################
# Confidence interval #
#######################

 mu_vals <- seq(mu_mle * 0.8, mu_mle * 1.2, length.out = 20)
 lambda_vals <- seq(lambda_mle * 0.8, lambda_mle * 1.2, length.out = 20)
 
df <- expand.grid(target_mu = mu_vals, target_lambda = lambda_vals)

df$target_mu
df$target_lambda

# Profile likelihood function for mean = lambda * mu
profile_logLik_2fix <- function(target_mu, target_lambda, data) {
  obj_fn <- function(par) {
    mu <- target_mu
    lambda <- target_lambda
    sigma <- par
  
    
    if (lambda <= 0 || sigma <= 0) return(Inf)
    return(cpn_neg_log_likelihood(c(lambda, mu, sigma), data))
  }
  
  opt <- optim(par = c(sigma_mle),
               fn = obj_fn,
               method = "Nelder-Mead")
  return(-opt$value)
}

df$LL <-NULL
for (i in 1:nrow(df)){
  target_mu <-  df$target_mu[i]
  target_lambda <-  df$target_lambda[i]
  df$LL[i] <-  profile_logLik_2fix(target_mu, target_lambda, sim_data)
}

df$mean=df$target_mu * df$target_lambda
df$mean

# Step 5: Compute profile likelihood ratio statistic
df$LR  <- -2 * (df$LL - logLik_mle)

# Step 6: 95% chi-squared threshold
threshold <- qchisq(0.95, df = 1)


# Step 7: Find confidence interval
ci_inds <- which(df$LR <= threshold)
ci_mean_lower <- min(df$mean[ci_inds])
ci_mean_upper <- max(df$mean[ci_inds])



# Report
cat("95% CI for lambda * mu:", ci_mean_lower, "-", ci_mean_upper, "\n")


# Assuming it is a normal distribution
# Calculate the sample mean
n <-length(sim_data)
sample_mean <- mean(sim_data)

# 3. Calculate the standard error of the mean
se <- sd(sim_data) / sqrt(n)

# 4. Get the 95% confidence interval using the t-distribution
alpha <- 0.05
t_crit <- qt(1 - alpha/2, df = n - 1)  # Critical value from t-distribution

ci_lower <- sample_mean - t_crit * se
ci_upper <- sample_mean + t_crit * se

# 5. Print results
cat("Sample mean assuming Normal distr.:", sample_mean, "\n")
cat("95% CI for the mean assuming Normal distr.:", ci_lower, "-", ci_upper, "\n")  

cat("Estimated mean from CPN model:", mle_estimates[1] * mle_estimates[2], "\n")
cat("95% CI for mean from CPN model:", ci_mean_lower, "-", ci_mean_upper, "\n")




# Suppose this is your one sample of Normal contributions
X_obs <- sim_data  # observed X_i values
N_obs <- length(X_obs)  # Observed N from Poisson

# Bootstrap settings
n_boot <- 10000
bootstrap_S <- replicate(n_boot, mean(sample(X_obs, size = N_obs, replace = TRUE)))

# Bootstrap CI (percentile method)
alpha <- 0.05
CI <- quantile(bootstrap_S, probs = c(alpha / 2, 1 - alpha / 2))

# Results
cat("Bootstrap 95% CI for total sum S from one compound sample:\n")
print(round(CI, 3))

################################
# Comparing two datasets       #
################################

rm(sim_data)
set.seed(123)  # Set seed for reproducibility
sim_data1 <- simulate_cpn(n = 200, lambda = 1.3, mu = 2, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution
sim_data2 <- simulate_cpn(n = 100, lambda = 1, mu = 2, sigma = 2)  # Generate 1000 observations from a Compound Poisson-Normal distribution

# Combine data and group labels
sim_data_pooled <- c(sim_data1, sim_data2)
group <- factor(c(rep(1, length(sim_data1)), rep(2, length(sim_data2))))


# ------- CPN--------------------------------
df_sim <- data.frame( y=sim_data_pooled,
                      group=group)

cpn_fit <- cpn(y~group, data=df_sim)
summary(cpn_fit)
#--------------------------------------------


# --- Null Model: shared lambda, mu and shared sigma ---
mle_estimates_pooled <- cpn_mle(sim_data_pooled)  # Estimate lambda, mu, and sigma using MLE


cat("Estimated lambda:", mle_estimates_pooled[1], "\n")
cat("Estimated mu:", mle_estimates_pooled[2], "\n")
cat("Estimated sigma:", mle_estimates_pooled[3], "\n")
cat("Estimated mean:", mle_estimates_pooled[1] * mle_estimates_pooled[2], "\n")


# Extract MLEs
lambda_mle_pooled <- mle_estimates_pooled[1]
mu_mle_pooled <- mle_estimates_pooled[2]
sigma_mle_pooled <- mle_estimates_pooled[3]

# Compute maximum log-likelihood at MLE
logL_null <- -cpn_neg_log_likelihood(c(lambda_mle_pooled,
                                             mu_mle_pooled,
                                             sigma_mle_pooled),
                                           sim_data_pooled)
logL_null





# --- Alternative Model: different lambdas, shared mu and sigma ---
neg_log_lik_alt <- function(params, data, group) {
  lambda1 <- params[1]
  lambda2 <- params[2]
  mu <- params[3]
  sigma <- params[4]

  likelihoods <- mapply(function(x, g) {
    
    lambda <- if (g == 1) lambda1 else lambda2
    
    if (lambda <= 0 || sigma <= 0) return(Inf)
    
    if (x == 0) {
      prob <- dpois(0, lambda)
    } else {
      k_max <- 10
      k_vals <- 1:k_max
      poisson_probs <- dpois(k_vals, lambda)
      normal_probs <- dnorm(x, mean = k_vals * mu, sd = sqrt(k_vals * sigma^2))
      prob <- sum(poisson_probs * normal_probs)
    }
    
    return(prob)
    
  }, x = data, g = group)
    

  return(-sum(log(likelihoods))) 

}

init_alt <- c(1, 1,
              mean(sim_data_pooled), sd(sim_data_pooled))

fit_alt <- optim(
  par = init_alt,
  fn = neg_log_lik_alt,
  data = sim_data_pooled,
  group = group,
  method = "Nelder-Mead"
)

logL_alt <- -fit_alt$value


# --- LRT ---
lrt_stat <- 2 * (logL_alt - logL_null)
p_value <- pchisq(lrt_stat, df = 1, lower.tail = FALSE)

# Output
cat("Log-likelihood (Null):", logL_null, "\n")
cat("Log-likelihood (Alt):", logL_alt, "\n")
cat("LRT Statistic:", lrt_stat, "\n")
cat("p-value:", p_value, "\n")




# Comparibg to t-test
# T-test expected to show a higher p-value
t.test(sim_data1, sim_data2)


# Comparibg with poisson distribution
# p-value should be similar

set.seed(123)
pois_data1 <- rpois(100, lambda = 1.4)  # First dataset with true λ = 5
pois_data2 <- rpois(100, lambda = 1)  # Second dataset with true λ = 7

pois_data <- data.frame(
  counts = c(pois_data1, pois_data2),  # Response variable (Poisson-distributed counts)
  group = rep(c("Dataset1", "Dataset2"), c(length(pois_data1), length(pois_data2)))  # Grouping variable
)

model <- glm(counts ~ group, family = poisson, data = pois_data)
summary_result <- summary(model)
lambda1_glm <- exp(coef(model)[1])  # Intercept = log(lambda1), so exp() gives lambda1
lambda1_glm 
lambda2_glm <- exp(coef(model)[1] + coef(model)[2])  # Lambda2 from group coefficient
lambda2_glm 
p_value_glm <- summary_result$coefficients["groupDataset2", "Pr(>|z|)"]
p_value_glm

##############################
# Difference in means and CI #
##############################

lambda1_hat <- fit_alt$par[1]
lambda2_hat <- fit_alt$par[2]
mu_hat <- fit_alt$par[3]
sigma_hat <- fit_alt$par[4]
mean1_hat <- lambda1_hat * mu_hat
mean2_hat <- lambda2_hat * mu_hat
mean_diff <- mean1_hat - mean2_hat


n1 <- sum(group == 1)
n2 <- sum(group == 2)

var1_hat <- lambda1_hat * (mu_hat^2 + sigma_hat*2)
var2_hat <- lambda2_hat * (mu_hat^2 + sigma_hat*2)

#calculate pooled standard deviation
pooled <- sqrt(((n1-1)*var1_hat + (n2-1)*var2_hat) / (n1+n1-2))

#view pooled standard deviation
pooled

# Standard error of the difference
se_diff <- pooled * sqrt(1 / n1 + 1 / n2)

# 95% confidence interval
z_value <- qnorm(0.975)  # 1.96 for 95% CI
ci_lower <- mean_diff - z_value * se_diff
ci_upper <- mean_diff + z_value * se_diff

# Output
cat("Mean Difference:", mean_diff, "\n")
cat("95% Confidence Interval: [", ci_lower, ",", ci_upper, "]\n")

# Bootstrap CI
boot_diff <- replicate(10000, {
  y1b <- sample(sim_data1, replace = TRUE)
  y2b <- sample(sim_data2, replace = TRUE)
  mean(y1b) - mean(y2b)
})

# 95% CI using percentile method
ci <- quantile(boot_diff, c(0.025, 0.975))
ci


#####################
# Plot the CPN data #
#####################

combined_data <- data.frame(
  value = c(sim_data1, sim_data2),  # Response variable (Poisson-distributed counts)
  dataset = rep(c("Dataset1", "Dataset2"), c(length(sim_data1), length(sim_data2)))  # Grouping variable
)

mle_estimates1 <- cpn_mle(sim_data1)  # Estimate lambda, mu, and sigma using MLE
mle_estimates1

mle_estimates2 <- cpn_mle(sim_data2)  # Estimate lambda, mu, and sigma using MLE
mle_estimates2
# Combine both datasets into one for plotting
summary_data <- data.frame(
  dataset = c("Dataset1", "Dataset2"),
  mean = c(mle_estimates1[1] * mle_estimates1[2], mle_estimates2[1] * mle_estimates2[2])
)


ggplot(combined_data, aes(x = dataset, y = value, color = dataset)) +
  geom_point(position = position_jitter(width = 0.2, height = 0), size = 2, alpha = 0.6) +  # Jitter points
  geom_point(data = summary_data, aes(x = dataset, y = mean), color = "black", size = 5, shape = 18) +  # Mean points
# geom_pointrange(data = summary_data, aes(x = dataset, y = mean, ymin = lower, ymax = upper), color = "black", size = 0.5)  +
  labs(
    title = "CPN Data",
    x = "Dataset",
    y = "Value"
  ) +
  scale_color_manual(values = c("blue", "red")) +
  theme_minimal()

